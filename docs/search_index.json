[
["index.html", "Preface", " Preface The purpose of this book is to describe: 1. What is a PolyGenic Score (PGS) 2. How Different Methods Produce a PGS 3. How Accurate are PGSs 4. What Caveats Need to Be Thought About in PGS Construction Please note the book is still in the early phases. "],
["introduction.html", "1 Introduction", " 1 Introduction PolyGenic Scores provide a key formulation of genetic findings into disease prediction. There has been a fair amount of concern over the accuracy, confounding and equitability of PGSs. I believe the best way to judge the significance of these worries, and more importantly, ultimately work towards their resolve, is through well-doccumented research. Quick note on set-up. I am writing this book within a sub-directory of my larger project directory. Hopefully everything will work this way. "],
["quality-control.html", "2 Quality Control 2.1 Previously Applied QC 2.2 Initial QC", " 2 Quality Control To begin work on producing a set of PGSs, the input data must first be carefully controlled. Throughout this project the UK Biobank will be the solitary set of data examined. The reason being is that the UK Biobank is the only large and easily accessed data-set that contains both genotypic and phenotypic information. A brief description of the UK Biobank … 2.1 Previously Applied QC In the process of sequencing the data the good people at the UK Biobank have already applied some quality control measures. To understand exactly what they did we first need to know how they even got the data to begin with. Of the nearly 500,000 people that were avaliable to be sequenced, batches of approximitley 4,600 people were created. The first 11 batches were sequenced on the UK BiLEVE Axiom array, the remaining 95 batches were sequenced with the UK Biobank Axiom Array. Both arrays were carefully created for this type of research, although we should be careful going forward as batch effects could change allele frequencies and the ultimate PGS distribution. As the arrays were novel, some of the allele probes were not constructed well enough to clearly determine what allele a person was. A total of 35,014 unique markers were therefore removed from everyone right off the bat. Genotype calling by Affymetrix resulted in a data set of 489,212 individuals typed at 812,428 markers with which to carry out further QC. This QC was population structure aware, with different ancestry groups determined by comparing PCA loadings from UK Biobank individuals to a selection of 1000 Genomes Project individuals (The markers that went into this PCA were carefully QC’d). Afterwards, within population structure homogenous groups to each batch the following tests were applied: Batch Effects Plate Effects Departures from Hardy-Weinberg Equilibrium Sex Effects Array Effects Discordance Across Controls If a marker failed the first 4 tests it was removed from the batch, and if a marker failed the last 2 tests then it was removed from the entire data-set. Each of these tests are hypothesis tests, with the cut-off chosen for removal set at p &lt; 10^-12. Clearly this is pretty extreme so it is important to note there may be more QC that needs to be done. fail Following marker QC a pipeline of sample QC was applied. A good description of the pipeline is shown below. What is important to note is that the doccumentation seems to note that only individuals with really extreme missingness or hetrozygosity were removed from the dataset. Otherwise they were simply marked as being an outlier in their respective test. A more stringent threshold of the sample QC and marker QC was set for the dataset that was used within a principal component analysis. The output of this PCA was then used to determine a white, British ancestry subset. This was completed by comparing the samples who declared they were white and British and then removing outliers from the main cluster of this grouping based on PCA. PCA Far more information on all of these steps can be found from the BioRxiv pdf: “Genome-wide geneticdata on ~500,000 UK Biobank participants”. The big takeaways that I gleam are that the UK Biobank did think about genotyping seriously, the most abberant markers have been removed although some questionable markers likely remain, and that quite a few abberant samples remain although removing them should be easy since they have been marked. All of this gives a good starting point for this work. 2.2 Initial QC To increase levels of QC, a file that lists all of the outliers was used to create a list of high quality anonymous IDS. The script that ran this entire process is displayed below. library(vroom) # Column titles for the qc files #19 = outliers in hetrozygosity #20 = putative sex chromosome aneuploidy #21 = in kinship table #22 = excluded from kinship calcs #23 = excess relatives #24 = British ancestry #25 = Used in PCA #26 = PCs #read in qc and the fam file, which contains the eids of the qc file qc &lt;- as.data.frame(vroom(&quot;~/athena/ukbiobank/qc/ukb_sample_qc.txt&quot;, delim = &quot; &quot;, col_names = F)) fam &lt;- read.table(&quot;~/athena/ukbiobank/calls/ukbb.22.fam&quot;, stringsAsFactors = F) #do brit thing brit_fam &lt;- fam[qc[,19] == 0 &amp; qc[,20] == 0 &amp; qc[,23] == 0 &amp; qc[,24] == 1,] brit_fam$eth &lt;- 0 brit_fam$eth_name &lt;- &quot;brit&quot; #read in the phen phen &lt;- read.csv(&quot;~/athena/ukbiobank/phenotypes/ukb26867.csv.gz&quot;, stringsAsFactors = F) #subset according to the specified conditions within &quot;Genetics of 38 blood and urine biomarkers in the UK Biobank&quot; good_fam &lt;- fam[qc[,19] == 0 &amp; qc[,20] == 0 &amp; qc[,23] == 0,] #subset the phen, qc, and fam files based on the good_fam above phen$eid[is.na(phen$eid)] &lt;- -999 phen &lt;- phen[phen[,1] %in% good_fam[,1],] qc &lt;- qc[fam[,1] %in% phen[,1],] fam &lt;- fam[fam[,1] %in% phen[,1],] phen &lt;- phen[order(phen[,1])[rank(fam[,1])],] #create vector for the survey answers of self-declared race ethnic &lt;- as.numeric(phen[,1288,drop=T]) ethnic_code &lt;- rep(0, length(ethnic)) ethnic_code[ethnic %in% c(1, 1001, 1002, 1003)] &lt;- 1 #european ethnic_code[ethnic %in% c(4002, 4003, 4)] &lt;- 2 #black ethnic_code[ethnic %in% c(3, 5, 3001, 3002, 3003, 3004)] &lt;- 3 #asian #do the kmeans and then calculate distance of each eid to its respective center euc_dist &lt;- function(x1, x2) sqrt(sum((x1 - x2) ^ 2)) pcs &lt;- qc[,26:65] set.seed(1) k_clu &lt;- kmeans(pcs, 3) if(file.exists(&quot;dist_to_center.RDS&quot;)){ dist_to_center &lt;- readRDS(&quot;dist_to_center.RDS&quot;) } else { dist_to_center &lt;- rep(0, length(ethnic_code)) for(i in 1:3){ for(j in which(k_clu$cluster == i)){ dist_to_center[j] &lt;- euc_dist(k_clu$centers[i,], pcs[j,]) } } saveRDS(dist_to_center, &quot;dist_to_center.RDS&quot;) } #determine cluster to ethnicity clu_eth_decode &lt;- c( names(sort(table(ethnic_code[k_clu$cluster == 1]), decreasing = T)[1]), names(sort(table(ethnic_code[k_clu$cluster == 2]), decreasing = T)[1]), names(sort(table(ethnic_code[k_clu$cluster == 3]), decreasing = T)[1])) clu_eth_decode[clu_eth_decode == &quot;0&quot;] &lt;- &quot;2&quot; #get the indices of the pca outliers ancestry_remove &lt;- list() for(i in 1:3){ cut_off &lt;- quantile(dist_to_center[k_clu$cluster == i], 0.99) ancestry_remove[[i]] &lt;- which(dist_to_center &gt; cut_off &amp; k_clu$cluster == i) } #assign ethnic groups then kick out outliers fam$eth_name &lt;- &quot;&quot; fam$eth_name[k_clu$cluster == which(clu_eth_decode == &quot;1&quot;)] &lt;- &quot;euro&quot; fam$eth_name[k_clu$cluster == which(clu_eth_decode == &quot;2&quot;)] &lt;- &quot;african&quot; fam$eth_name[k_clu$cluster == which(clu_eth_decode == &quot;3&quot;)] &lt;- &quot;asian&quot; fam$eth &lt;- 0 fam$eth[k_clu$cluster == which(clu_eth_decode == &quot;1&quot;)] &lt;- 1 fam$eth[k_clu$cluster == which(clu_eth_decode == &quot;2&quot;)] &lt;- 2 fam$eth[k_clu$cluster == which(clu_eth_decode == &quot;3&quot;)] &lt;- 3 fam &lt;- fam[-unlist(ancestry_remove),] phen &lt;- phen[-unlist(ancestry_remove),] #break out ancestry fams euro_fam &lt;- fam[fam$eth_name == &quot;euro&quot;,] african_fam &lt;- fam[fam$eth_name == &quot;african&quot;,] asian_fam &lt;- fam[fam$eth_name == &quot;asian&quot;,] #create special for not dead eids bad_bool &lt;- apply(phen[,which(colnames(phen) %in% c(&quot;X40000.0.0&quot;, &quot;X191.0.0&quot;))], 1, function(x) any(! (x == &quot;&quot;))) bad_eid &lt;- phen$eid[bad_bool] alive_fam &lt;- fam[!(fam[,1] %in% bad_eid),] alive_euro_fam &lt;- euro_fam[!(euro_fam[,1] %in% bad_eid),] alive_african_fam &lt;- african_fam[!(african_fam[,1] %in% bad_eid),] alive_asian_fam &lt;- asian_fam[!(asian_fam[,1] %in% bad_eid),] #write all of the tables write.table(fam, &quot;fam_files/qc_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) write.table(euro_fam, &quot;fam_files/qc_euro_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) write.table(african_fam, &quot;fam_files/qc_african_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) write.table(asian_fam, &quot;fam_files/qc_asian_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) write.table(alive_fam, &quot;fam_files/qc_alive_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) write.table(alive_euro_fam, &quot;fam_files/qc_alive_euro_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) write.table(alive_african_fam, &quot;fam_files/qc_alive_african_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) write.table(alive_asian_fam, &quot;fam_files/qc_alive_asian_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) write.table(brit_fam, &quot;fam_files/qc_brit_fam&quot;, row.names = F, col.names = F, quote = F, sep = &#39;\\t&#39;) "],
["summary-statistics.html", "3 Summary Statistics", " 3 Summary Statistics To create the PGSs we need summary statistics, or results from Genome-Wide Association Studies (GWASs). One overall requirement for the summary statistics that we use is that they cannot have utilized the UK Biobank in their production. If they did we would be overfitting the problem, which is not good. Each summary statistic must also have the following features (or columns): Chromosome Variant ID (the rsID) Effect Allele Alternate Allele Effect (Beta, Odds Ratio) P-Value A final consideration is to pull from summary statistics with the largest possibly underlying sample size, because it will produce the most accurate estimates. The studies can be obtained with: "]
]
